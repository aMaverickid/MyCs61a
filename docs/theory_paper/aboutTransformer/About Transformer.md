# About Transformer

refer to [TRANSFORMERS FROM SCRATCH](https://peterbloem.nl/blog/transformers)

## Self-attention

1. Self-attention is a sequence-to-sequence operation, è€Œä¸”input vectors $x_1, x_2, ...,x_t$å’Œoutput vectors$y_1, y_2,...,y_t$çš„dimensionç›¸åŒ
2. The output vector $y_i$ is a weighted average over all the input vectors

$$
y_i = \sum_j{w_{ij}x_j}
$$

3. So what is $w_{ij}$ ? The weight $w_{ij}$ is not a parameter, as in a normal neural net, but it is *derived* from a function over $x_i$ and $x_j$. The simplest option for this function is the dot product:

$$
W'_{ij} = x_i^Tx_j
$$

4. å½“ç„¶è¿˜è¦å¯¹ $W'_{ij}$ ä½œ Normalizationï¼Œ

$$
w_{ij} = \frac{e^{w_{ij}}}{\sum_j{e^{w'_{ij}}}}.
$$

![img](https://peterbloem.nl/files/transformers/self-attention.svg)

5. Summary: '*This is the only operation in the whole architecture that propagates information between vectors. Every other operation in the transformer is applied to each vector in the input sequence without interactions between vectors.*'

## Understanding why self-attention works !!!

### example: *movie recommendation*

***' we make the movie features and user features parameters of the model. We then ask users for a small number of movies that they like and we optimize the user features and movie features so that their dot product matches the known likes***

***Even though we donâ€™t tell the model what any of the features should mean, in practice, it turns out that after training the features do actually reflect meaningful semantics about the movie content.'***

### Same with the self-attention mechanism!

Self-attention å®é™…ä¸Šæ˜¯æ²¡æœ‰å‚æ•°(*for now*)çš„, åªæ˜¯å¯¹æ¯ä¸ªinput vectorï¼Œç”¨å®ƒä¸å…¶ä»–æ‰€æœ‰vectoræ±‚å†…ç§¯ï¼Œæ±‚ç›¸å…³åº¦ï¼Œç„¶ååŠ æƒäº§ç”Ÿæ–°çš„vectorçš„ä¸€ä¸ªå·¥å…·ã€‚

é‚£ä¹ˆæ¨¡å‹æ€ä¹ˆèƒ½åœ¨è®­ç»ƒä¸­è¶‹äºæœ‰æ•ˆå‘¢ï¼Ÿæ¨¡å‹è®­ç»ƒæ—¶ï¼Œä¼šæ ¹æ®Self-attentionæ±‚å‡ºçš„ç›¸å…³åº¦ç¬¦ä¸ç¬¦åˆå®é™…ç»“æœï¼Œè¿›è¡Œè°ƒæ•´ã€‚

é‚£ä¹ˆè°ƒæ•´çš„æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ**input sequence**.

å‡å¦‚æˆ‘ä»¬ç°åœ¨å¯¹ä¸€ä¸²æ–‡å­—ä½¿ç”¨self-attension,

 ***'we simply assign each word $t$ in our vocabulary an embedding vector $v_t$â€‹â€‹ (the values of which weâ€™ll learn). This is whatâ€™s known as an embedding layer in sequence modeling'***

å­¦çš„å°±æ˜¯ word embedding åçš„ $sequence\{V_t\}$

### Note

![image-20240322113023234](C:\Users\86198\AppData\Roaming\Typora\typora-user-images\image-20240322113023234.png)

## In Pytorch: basic self-attention

See `basic_self_attention.py`

## Additional tricks

> The actual self-attention used in modern transformers relies on three additional tricks. 

### 1) Queries, keys and values

åœ¨**basic self-attention**ä¸­ï¼Œinput vector $x_i$â€‹ is used in three different ways in the self attention operation:

- It is compared to every other vector to establish the weights for its own output $ğ²_i$
- It is compared to every other vector to establish the weights for the output of the j-th vector $y_j$
- It is used as part of the weighted sum to compute each output vector once the weights have been established

These roles are often called the **query**, the **key** and the **value**
$$
q_i = W_q x_{i} \qquad k_i = W_kx_i \qquad v_i = W_vx_i \\
w'_{ij} = \frac{q_i^Tk_j}{\sqrt{k}} \\
w_{ij} = softmax(w'_{ij}) \\
y_i = \sum_j{w'_{ij}v_i}
$$

### 2) Scaling the dot product

### 3) Multi-head attention

## In Pytorch: complete self-attention

See `complete_self_attention.py`

## Building *transformers*

> A transformer is not just a self-attention layer, it is an *architecture*
>
> The definition of the transformer architecture is vague, but here weâ€™ll use the following definition:
>
> > Any architecture designed to process a connected set of unitsâ€”such as the tokens in a sequence or the pixels in an imageâ€”where the only interaction between units is through self-attention.

**Transformers** å’Œ **Convolutions**ä¸€æ ·ï¼Œhave a *standard approach to build self-attention layers up into a larger network*. The first step is to wrap the self-attention into a *block* that we can repeat.

## The transformer block

#### General structure

![img](https://peterbloem.nl/files/transformers/transformer-block.svg)

The block applies, in sequence: a self attention layer, layer normalization, a feed forward layer (a single MLP applied independently to each vector), and another layer normalization. Residual connections are added around both, before the normalization

> ï¼*Normalization and residual connections are standard tricks used to help deep neural networks train faster and more accurately. The layer normalization is applied over the embedding dimension only.*  éœ€è¦å†å»äº†è§£ä¸€äº›ï¼Œè¿™é‡Œå°±ç›´æ¥æ‹¿æ¥ç”¨

**Implementation:** *also see in `transformers_block.py`*

```python
class TransformerBlock(nn.Module):
    def __init__(self, k, heads):
        super().__init__()
    
        self.attention = SelfAttention(k, heads)

        self.norm1 = nn.LayerNorm(k)
        self.norm2 = nn.LayerNorm(k)

        self.feedforward = nn.Sequential(
            nn.Linear(k, 4*k)
            nn.ReLU(),
            nn.Linear(4 * k, k)
        )

    def forward(self, x):
        attended = self.attention(x)
        x = self.norm1(attended + x)

        fedforward = self.feedforward(x)

        return self.norm2(fedforward + x)
```

> Weâ€™ve made the relatively arbitrary choice of making the hidden layer of the feedforward 4 times as big as the input and output. Smaller values may work as well, and save memory, but it should be bigger than the input/output layers. 

>  å…³äºMLP ã€Hidden Layerå‚æ•°æ€ä¹ˆé€‰æœ‰ç‚¹å¿˜äº†ï¼Œå…ˆæ‹¿æ¥ç”¨

### Our first application: Classification transformer

> The simplest transformer we can build is a *sequence classifier*. *Weâ€™ll use the IMDb sentiment classification dataset: the instances are movie reviews, tokenized into sequences of words, and the classification labels are `positive` and `negative` (indicating whether the review was positive or negative about the movie)*

**The General Idea:** Use a large chain of transformer blocks to extract the information in the movie reviews. Feed it the input sequence of words from the tokenized movie reviews,  transformer blocks would produce a output sequence, then do something to it to get a single classification.

>  *The most common way to build a sequence classifier out of sequence-to-sequence layers, is to apply global average pooling to the final output sequence, and to map the result to a softmaxed class vector.*

![img](https://peterbloem.nl/files/transformers/classifier.svg)

> *The most common way to build a sequence classifier out of sequence-to-sequence layers, is to apply global average pooling to the final output sequence, and to map the result to a softmaxed class vector.*

#### Input: using the positions

ä¹‹å‰æˆ‘ä»¬æåˆ°ï¼Œself-attentionæ˜¯ permutation invariantçš„ï¼ˆå³input sequence çš„vector é¡ºåºæ”¹å˜ï¼Œå¹¶ä¸ä¼šå½±å“æ¯ä¸ªvector æœ€å output å‡ºçš„ç»“æœï¼‰ï¼Œç„¶å transformer block çš„ å…¶ä»–å±‚ *layer normalization, a feed forward layer* ä¹Ÿéƒ½æ˜¯ permutation invariant çš„ã€‚è¿™å°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ç°åœ¨çš„transformer block å¯¹äºè¯åºä¸åŒçš„ä¸¤å¥è¯ï¼Œæœ€åè¾“å‡ºçš„classification ç»“æœæ˜¯ç›¸åŒçš„ã€‚ æ˜¾ç„¶å¯¹äºäººç±»è¯­è¨€æ¥è¯´ï¼Œä¸€å¥è¯ä¸­çš„è¯åºå˜åŒ–éå¸¸å½±å“ä¸€å¥è¯çš„è¯­æ„ã€‚å› æ­¤éœ€è¦æ”¹è¿›ã€‚

> *Clearly, we want our state-of-the-art language model to have at least some **sensitivity to word order**, so this needs to be fixed.*
>
>*The solution is simple: we create a second vector of equal length, that represents the position of the word in the current sentence, and add this to the word embedding. There are two options.*

##### option 1: position embeddings 

*' We simply embed the positions like we did the words. Just like we created embedding vectors ğ¯cat and ğ¯susan, we create embedding vectors ğ¯12 and ğ¯25. Up to however long we expect sequences to get. The drawback is that we have to see sequences of every length during training, otherwise the relevant position embeddings don't get trained. The benefit is that it works pretty well, and it's easy to implement.'*

##### option 2: position encodings

*' Position encodings work in the same way as embeddings, except that we don't learn the position vectors, we just choose some function $f:â„•â†’â„^k$â€‹ to map the positions to real valued vectors, and let the network figure out how to interpret these encodings. The benefit is that for a well chosen function, the network should be able to deal with sequences that are longer than those it's seen during training (it's unlikely to perform well on them, but at least we can check). The drawbacks are that the choice of encoding function is a complicated hyperparameter, and it complicates the implementation a little.'*

> noteï¼šä¸å¤ªæ‡‚ä¸¤è€…åŒºåˆ«ï¼Œä¸ºä»€ä¹ˆ position encoding ä¸ç”¨ å­¦ position vectorï¼Ÿ

**For the sake of simplicity, we'll use position embeddings in our implementation**



![image-20240326165411387](C:\Users\86198\AppData\Roaming\Typora\typora-user-images\image-20240326165411387.png)





















