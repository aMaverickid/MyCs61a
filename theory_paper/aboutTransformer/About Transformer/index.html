
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../../../Courses/C%2B%2B/Memory%20Model%20in%20C%2B%2B/">
      
      
      
      <link rel="icon" href="../../../assets/head.jpg">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.22">
    
    
      
        <title>About Transformer - Maverickid's Notes</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.732c4fb1.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../css/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#about-transformer" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Maverickid&#39;s Notes" class="md-header__button md-logo" aria-label="Maverickid's Notes" data-md-component="logo">
      
  <img src="../../../assets/head.jpg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Maverickid's Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              About Transformer
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Maverickid&#39;s Notes" class="md-nav__button md-logo" aria-label="Maverickid's Notes" data-md-component="logo">
      
  <img src="../../../assets/head.jpg" alt="logo">

    </a>
    Maverickid's Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    è¯¾ç¨‹ç¬”è®°
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            è¯¾ç¨‹ç¬”è®°
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/CS61A/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CS61A
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    ADS
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            ADS
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week6/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week6
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week7/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week7
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week8/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week8
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/midtermReview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    midTerm Review
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week9.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week9
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week10
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week11
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week12.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week12
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    è®¡ç»„
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            è®¡ç»„
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/Computer_Organization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    intro
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/Computer_Organization/Chapter2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Chapter2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/Computer_Organization/Chapter3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Chapter3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/Computer_Organization/Exception/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Exception
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    DB
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            DB
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/DataBaseSystem/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    intro
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/DataBaseSystem/Lecture5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lecture5
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/C%2B%2B/Memory%20Model%20in%20C%2B%2B/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    C++
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Other
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Other
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    About Transformer
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    About Transformer
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Self-attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#understanding-why-self-attention-works" class="md-nav__link">
    <span class="md-ellipsis">
      Understanding why self-attention works !!!
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Understanding why self-attention works !!!">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-movie-recommendation" class="md-nav__link">
    <span class="md-ellipsis">
      example: movie recommendation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#same-with-the-self-attention-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      Same with the self-attention mechanism!
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#note" class="md-nav__link">
    <span class="md-ellipsis">
      Note
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#in-pytorch-basic-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      In Pytorch: basic self-attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-tricks" class="md-nav__link">
    <span class="md-ellipsis">
      Additional tricks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Additional tricks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-queries-keys-and-values" class="md-nav__link">
    <span class="md-ellipsis">
      1) Queries, keys and values
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-scaling-the-dot-product" class="md-nav__link">
    <span class="md-ellipsis">
      2) Scaling the dot product
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-multi-head-attention" class="md-nav__link">
    <span class="md-ellipsis">
      3) Multi-head attention
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#in-pytorch-complete-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      In Pytorch: complete self-attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#building-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Building transformers
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-transformer-block" class="md-nav__link">
    <span class="md-ellipsis">
      The transformer block
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The transformer block">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-structure" class="md-nav__link">
    <span class="md-ellipsis">
      General structure
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#our-first-application-classification-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      Our first application: Classification transformer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Our first application: Classification transformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#input-using-the-positions" class="md-nav__link">
    <span class="md-ellipsis">
      Input: using the positions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Input: using the positions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#option-1-position-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      option 1: position embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#option-2-position-encodings" class="md-nav__link">
    <span class="md-ellipsis">
      option 2: position encodings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-generate-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Text-generate Transformers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-global-context-aware-transformer-for-language-guided-video-segmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Local-Global Context Aware Transformer for Language-Guided Video Segmentation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Self-attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#understanding-why-self-attention-works" class="md-nav__link">
    <span class="md-ellipsis">
      Understanding why self-attention works !!!
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Understanding why self-attention works !!!">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-movie-recommendation" class="md-nav__link">
    <span class="md-ellipsis">
      example: movie recommendation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#same-with-the-self-attention-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      Same with the self-attention mechanism!
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#note" class="md-nav__link">
    <span class="md-ellipsis">
      Note
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#in-pytorch-basic-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      In Pytorch: basic self-attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-tricks" class="md-nav__link">
    <span class="md-ellipsis">
      Additional tricks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Additional tricks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-queries-keys-and-values" class="md-nav__link">
    <span class="md-ellipsis">
      1) Queries, keys and values
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-scaling-the-dot-product" class="md-nav__link">
    <span class="md-ellipsis">
      2) Scaling the dot product
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-multi-head-attention" class="md-nav__link">
    <span class="md-ellipsis">
      3) Multi-head attention
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#in-pytorch-complete-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      In Pytorch: complete self-attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#building-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Building transformers
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-transformer-block" class="md-nav__link">
    <span class="md-ellipsis">
      The transformer block
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The transformer block">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-structure" class="md-nav__link">
    <span class="md-ellipsis">
      General structure
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#our-first-application-classification-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      Our first application: Classification transformer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Our first application: Classification transformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#input-using-the-positions" class="md-nav__link">
    <span class="md-ellipsis">
      Input: using the positions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Input: using the positions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#option-1-position-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      option 1: position embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#option-2-position-encodings" class="md-nav__link">
    <span class="md-ellipsis">
      option 2: position encodings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-generate-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Text-generate Transformers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-global-context-aware-transformer-for-language-guided-video-segmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Local-Global Context Aware Transformer for Language-Guided Video Segmentation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="about-transformer">About Transformer</h1>
<p>refer to <a href="https://peterbloem.nl/blog/transformers">TRANSFORMERS FROM SCRATCH</a></p>
<h2 id="self-attention">Self-attention</h2>
<ol>
<li>Self-attention is a sequence-to-sequence operation, è€Œä¸”input vectors <span class="arithmatex">\(x_1, x_2, ...,x_t\)</span>å’Œoutput vectors<span class="arithmatex">\(y_1, y_2,...,y_t\)</span>çš„dimensionç›¸åŒ</li>
<li>The output vector <span class="arithmatex">\(y_i\)</span> is a weighted average over all the input vectors</li>
</ol>
<div class="arithmatex">\[
y_i = \sum_j{w_{ij}x_j}
\]</div>
<ol>
<li>So what is <span class="arithmatex">\(w_{ij}\)</span> ? The weight <span class="arithmatex">\(w_{ij}\)</span> is not a parameter, as in a normal neural net, but it is <em>derived</em> from a function over <span class="arithmatex">\(x_i\)</span> and <span class="arithmatex">\(x_j\)</span>. The simplest option for this function is the dot product:</li>
</ol>
<div class="arithmatex">\[
W'_{ij} = x_i^Tx_j
\]</div>
<ol>
<li>å½“ç„¶è¿˜è¦å¯¹ <span class="arithmatex">\(W'_{ij}\)</span> ä½œ Normalizationï¼Œ</li>
</ol>
<div class="arithmatex">\[
w_{ij} = \frac{e^{w_{ij}}}{\sum_j{e^{w'_{ij}}}}.
\]</div>
<p><img alt="img" src="https://peterbloem.nl/files/transformers/self-attention.svg" /></p>
<ol>
<li>Summary: '<em>This is the only operation in the whole architecture that propagates information between vectors. Every other operation in the transformer is applied to each vector in the input sequence without interactions between vectors.</em>'</li>
</ol>
<h2 id="understanding-why-self-attention-works">Understanding why self-attention works !!!</h2>
<h3 id="example-movie-recommendation">example: <em>movie recommendation</em></h3>
<p><strong><em>' we make the movie features and user features parameters of the model. We then ask users for a small number of movies that they like and we optimize the user features and movie features so that their dot product matches the known likes</em></strong></p>
<p><strong><em>Even though we donâ€™t tell the model what any of the features should mean, in practice, it turns out that after training the features do actually reflect meaningful semantics about the movie content.'</em></strong></p>
<h3 id="same-with-the-self-attention-mechanism">Same with the self-attention mechanism!</h3>
<p>Self-attention å®é™…ä¸Šæ˜¯æ²¡æœ‰å‚æ•°(<em>for now</em>)çš„, åªæ˜¯å¯¹æ¯ä¸ªinput vectorï¼Œç”¨å®ƒä¸å…¶ä»–æ‰€æœ‰vectoræ±‚å†…ç§¯ï¼Œæ±‚ç›¸å…³åº¦ï¼Œç„¶ååŠ æƒäº§ç”Ÿæ–°çš„vectorçš„ä¸€ä¸ªå·¥å…·ã€‚</p>
<p>é‚£ä¹ˆæ¨¡å‹æ€ä¹ˆèƒ½åœ¨è®­ç»ƒä¸­è¶‹äºæœ‰æ•ˆå‘¢ï¼Ÿæ¨¡å‹è®­ç»ƒæ—¶ï¼Œä¼šæ ¹æ®Self-attentionæ±‚å‡ºçš„ç›¸å…³åº¦ç¬¦ä¸ç¬¦åˆå®é™…ç»“æœï¼Œè¿›è¡Œè°ƒæ•´ã€‚</p>
<p>é‚£ä¹ˆè°ƒæ•´çš„æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ<strong>input sequence</strong>.</p>
<p>å‡å¦‚æˆ‘ä»¬ç°åœ¨å¯¹ä¸€ä¸²æ–‡å­—ä½¿ç”¨self-attension,</p>
<p><strong><em>'we simply assign each word <span class="arithmatex">\(t\)</span> in our vocabulary an embedding vector <span class="arithmatex">\(v_t\)</span>â€‹â€‹ (the values of which weâ€™ll learn). This is whatâ€™s known as an embedding layer in sequence modeling'</em></strong></p>
<p>å­¦çš„å°±æ˜¯ word embedding åçš„ <span class="arithmatex">\(sequence\{V_t\}\)</span></p>
<h3 id="note">Note</h3>
<p><img alt="image-20240322113023234" src="C:\Users\86198\AppData\Roaming\Typora\typora-user-images\image-20240322113023234.png" /></p>
<h2 id="in-pytorch-basic-self-attention">In Pytorch: basic self-attention</h2>
<p>See <code>basic_self_attention.py</code></p>
<h2 id="additional-tricks">Additional tricks</h2>
<blockquote>
<p>The actual self-attention used in modern transformers relies on three additional tricks. </p>
</blockquote>
<h3 id="1-queries-keys-and-values">1) Queries, keys and values</h3>
<p>åœ¨<strong>basic self-attention</strong>ä¸­ï¼Œinput vector <span class="arithmatex">\(x_i\)</span>â€‹ is used in three different ways in the self attention operation:</p>
<ul>
<li>It is compared to every other vector to establish the weights for its own output <span class="arithmatex">\(ğ²_i\)</span></li>
<li>It is compared to every other vector to establish the weights for the output of the j-th vector <span class="arithmatex">\(y_j\)</span></li>
<li>It is used as part of the weighted sum to compute each output vector once the weights have been established</li>
</ul>
<p>These roles are often called the <strong>query</strong>, the <strong>key</strong> and the <strong>value</strong>
$$
q_i = W_q x_{i} \qquad k_i = W_kx_i \qquad v_i = W_vx_i \
w'<em>{ij} = \frac{q_i^Tk_j}{\sqrt{k}} \
w</em>{ij} = softmax(w'<em>{ij}) \
y_i = \sum_j{w'</em>{ij}v_i}
$$</p>
<h3 id="2-scaling-the-dot-product">2) Scaling the dot product</h3>
<h3 id="3-multi-head-attention">3) Multi-head attention</h3>
<h2 id="in-pytorch-complete-self-attention">In Pytorch: complete self-attention</h2>
<p>See <code>complete_self_attention.py</code></p>
<h2 id="building-transformers">Building <em>transformers</em></h2>
<blockquote>
<p>A transformer is not just a self-attention layer, it is an <em>architecture</em></p>
<p>The definition of the transformer architecture is vague, but here weâ€™ll use the following definition:</p>
<blockquote>
<p>Any architecture designed to process a connected set of unitsâ€”such as the tokens in a sequence or the pixels in an imageâ€”where the only interaction between units is through self-attention.</p>
</blockquote>
</blockquote>
<p><strong>Transformers</strong> å’Œ <strong>Convolutions</strong>ä¸€æ ·ï¼Œhave a <em>standard approach to build self-attention layers up into a larger network</em>. The first step is to wrap the self-attention into a <em>block</em> that we can repeat.</p>
<h2 id="the-transformer-block">The transformer block</h2>
<h4 id="general-structure">General structure</h4>
<p><img alt="img" src="https://peterbloem.nl/files/transformers/transformer-block.svg" /></p>
<p>The block applies, in sequence: a self attention layer, layer normalization, a feed forward layer (a single MLP applied independently to each vector), and another layer normalization. Residual connections are added around both, before the normalization</p>
<blockquote>
<p>ï¼<em>Normalization and residual connections are standard tricks used to help deep neural networks train faster and more accurately. The layer normalization is applied over the embedding dimension only.</em>  éœ€è¦å†å»äº†è§£ä¸€äº›ï¼Œè¿™é‡Œå°±ç›´æ¥æ‹¿æ¥ç”¨</p>
</blockquote>
<p><strong>Implementation:</strong> <em>also see in <code>transformers_block.py</code></em></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">heads</span><span class="p">):</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">heads</span><span class="p">)</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">k</span><span class="p">)</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>        <span class="p">)</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>        <span class="n">attended</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">attended</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>        <span class="n">fedforward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">fedforward</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
</code></pre></div>
<blockquote>
<p>Weâ€™ve made the relatively arbitrary choice of making the hidden layer of the feedforward 4 times as big as the input and output. Smaller values may work as well, and save memory, but it should be bigger than the input/output layers. </p>
<p>å…³äºMLP ã€Hidden Layerå‚æ•°æ€ä¹ˆé€‰æœ‰ç‚¹å¿˜äº†ï¼Œå…ˆæ‹¿æ¥ç”¨</p>
</blockquote>
<h3 id="our-first-application-classification-transformer">Our first application: Classification transformer</h3>
<blockquote>
<p>The simplest transformer we can build is a <em>sequence classifier</em>. <em>Weâ€™ll use the IMDb sentiment classification dataset: the instances are movie reviews, tokenized into sequences of words, and the classification labels are <code>positive</code> and <code>negative</code> (indicating whether the review was positive or negative about the movie)</em></p>
</blockquote>
<p><strong>The General Idea:</strong> Use a large chain of transformer blocks to extract the information in the movie reviews. Feed it the input sequence of words from the tokenized movie reviews,  transformer blocks would produce a output sequence, then do something to it to get a single classification.</p>
<blockquote>
<p><em>The most common way to build a sequence classifier out of sequence-to-sequence layers, is to apply global average pooling to the final output sequence, and to map the result to a softmaxed class vector.</em></p>
</blockquote>
<p><img alt="img" src="https://peterbloem.nl/files/transformers/classifier.svg" /></p>
<blockquote>
<p><em>The most common way to build a sequence classifier out of sequence-to-sequence layers, is to apply global average pooling to the final output sequence, and to map the result to a softmaxed class vector.</em></p>
</blockquote>
<h4 id="input-using-the-positions">Input: using the positions</h4>
<p>ä¹‹å‰æˆ‘ä»¬æåˆ°ï¼Œself-attentionæ˜¯ permutation invariantçš„ï¼ˆå³input sequence çš„vector é¡ºåºæ”¹å˜ï¼Œå¹¶ä¸ä¼šå½±å“æ¯ä¸ªvector æœ€å output å‡ºçš„ç»“æœï¼‰ï¼Œç„¶å transformer block çš„ å…¶ä»–å±‚ <em>layer normalization, a feed forward layer</em> ä¹Ÿéƒ½æ˜¯ permutation invariant çš„ã€‚è¿™å°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ç°åœ¨çš„transformer block å¯¹äºè¯åºä¸åŒçš„ä¸¤å¥è¯ï¼Œæœ€åè¾“å‡ºçš„classification ç»“æœæ˜¯ç›¸åŒçš„ã€‚ æ˜¾ç„¶å¯¹äºäººç±»è¯­è¨€æ¥è¯´ï¼Œä¸€å¥è¯ä¸­çš„è¯åºå˜åŒ–éå¸¸å½±å“ä¸€å¥è¯çš„è¯­æ„ã€‚å› æ­¤éœ€è¦æ”¹è¿›ã€‚</p>
<blockquote>
<p><em>Clearly, we want our state-of-the-art language model to have at least some </em><em>sensitivity to word order</em><em>, so this needs to be fixed.</em></p>
<p><em>The solution is simple: we create a second vector of equal length, that represents the position of the word in the current sentence, and add this to the word embedding. There are two options.</em></p>
</blockquote>
<h5 id="option-1-position-embeddings">option 1: position embeddings</h5>
<p><em>' We simply embed the positions like we did the words. Just like we created embedding vectors ğ¯cat and ğ¯susan, we create embedding vectors ğ¯12 and ğ¯25. Up to however long we expect sequences to get. The drawback is that we have to see sequences of every length during training, otherwise the relevant position embeddings don't get trained. The benefit is that it works pretty well, and it's easy to implement.'</em></p>
<h5 id="option-2-position-encodings">option 2: position encodings</h5>
<p><em>' Position encodings work in the same way as embeddings, except that we don't learn the position vectors, we just choose some function <span class="arithmatex">\(f:â„•â†’â„^k\)</span>â€‹ to map the positions to real valued vectors, and let the network figure out how to interpret these encodings. The benefit is that for a well chosen function, the network should be able to deal with sequences that are longer than those it's seen during training (it's unlikely to perform well on them, but at least we can check). The drawbacks are that the choice of encoding function is a complicated hyperparameter, and it complicates the implementation a little.'</em></p>
<blockquote>
<p>noteï¼šä¸å¤ªæ‡‚ä¸¤è€…åŒºåˆ«ï¼Œä¸ºä»€ä¹ˆ position encoding ä¸ç”¨ å­¦ position vectorï¼Ÿ</p>
</blockquote>
<p><strong>For the sake of simplicity, we'll use position embeddings in our implementation</strong></p>
<h3 id="text-generate-transformers">Text-generate Transformers</h3>
<h3 id="local-global-context-aware-transformer-for-language-guided-video-segmentation">Local-Global Context Aware Transformer for Language-Guided Video Segmentation</h3>
<p><img alt="alt text" src="../image.png" /></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.5cfa9459.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>