
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../../../Courses/C%2B%2B/Memory%20Model%20in%20C%2B%2B/">
      
      
      
      <link rel="icon" href="../../../assets/head.jpg">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.22">
    
    
      
        <title>About Transformer - Maverickid's Notes</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.732c4fb1.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../css/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#about-transformer" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Maverickid&#39;s Notes" class="md-header__button md-logo" aria-label="Maverickid's Notes" data-md-component="logo">
      
  <img src="../../../assets/head.jpg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Maverickid's Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              About Transformer
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Maverickid&#39;s Notes" class="md-nav__button md-logo" aria-label="Maverickid's Notes" data-md-component="logo">
      
  <img src="../../../assets/head.jpg" alt="logo">

    </a>
    Maverickid's Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    课程笔记
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            课程笔记
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/CS61A/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CS61A
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    ADS
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            ADS
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week6/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week6
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week7/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week7
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week8/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week8
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/midtermReview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    midTerm Review
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week9.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week9
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week10
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week11
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/ADS/week12.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week12
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    计组
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            计组
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/Computer_Organization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    intro
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/Computer_Organization/Chapter2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Chapter2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/Computer_Organization/Chapter3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Chapter3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/Computer_Organization/Exception/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Exception
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    DB
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            DB
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/DataBaseSystem/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    intro
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/DataBaseSystem/Lecture5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lecture5
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/C%2B%2B/Memory%20Model%20in%20C%2B%2B/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    C++
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Other
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Other
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    About Transformer
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    About Transformer
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Self-attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#understanding-why-self-attention-works" class="md-nav__link">
    <span class="md-ellipsis">
      Understanding why self-attention works !!!
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Understanding why self-attention works !!!">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-movie-recommendation" class="md-nav__link">
    <span class="md-ellipsis">
      example: movie recommendation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#same-with-the-self-attention-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      Same with the self-attention mechanism!
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#note" class="md-nav__link">
    <span class="md-ellipsis">
      Note
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#in-pytorch-basic-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      In Pytorch: basic self-attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-tricks" class="md-nav__link">
    <span class="md-ellipsis">
      Additional tricks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Additional tricks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-queries-keys-and-values" class="md-nav__link">
    <span class="md-ellipsis">
      1) Queries, keys and values
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-scaling-the-dot-product" class="md-nav__link">
    <span class="md-ellipsis">
      2) Scaling the dot product
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-multi-head-attention" class="md-nav__link">
    <span class="md-ellipsis">
      3) Multi-head attention
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#in-pytorch-complete-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      In Pytorch: complete self-attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#building-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Building transformers
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-transformer-block" class="md-nav__link">
    <span class="md-ellipsis">
      The transformer block
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The transformer block">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-structure" class="md-nav__link">
    <span class="md-ellipsis">
      General structure
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#our-first-application-classification-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      Our first application: Classification transformer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Our first application: Classification transformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#input-using-the-positions" class="md-nav__link">
    <span class="md-ellipsis">
      Input: using the positions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Input: using the positions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#option-1-position-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      option 1: position embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#option-2-position-encodings" class="md-nav__link">
    <span class="md-ellipsis">
      option 2: position encodings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-generate-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Text-generate Transformers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-global-context-aware-transformer-for-language-guided-video-segmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Local-Global Context Aware Transformer for Language-Guided Video Segmentation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Self-attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#understanding-why-self-attention-works" class="md-nav__link">
    <span class="md-ellipsis">
      Understanding why self-attention works !!!
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Understanding why self-attention works !!!">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-movie-recommendation" class="md-nav__link">
    <span class="md-ellipsis">
      example: movie recommendation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#same-with-the-self-attention-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      Same with the self-attention mechanism!
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#note" class="md-nav__link">
    <span class="md-ellipsis">
      Note
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#in-pytorch-basic-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      In Pytorch: basic self-attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-tricks" class="md-nav__link">
    <span class="md-ellipsis">
      Additional tricks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Additional tricks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-queries-keys-and-values" class="md-nav__link">
    <span class="md-ellipsis">
      1) Queries, keys and values
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-scaling-the-dot-product" class="md-nav__link">
    <span class="md-ellipsis">
      2) Scaling the dot product
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-multi-head-attention" class="md-nav__link">
    <span class="md-ellipsis">
      3) Multi-head attention
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#in-pytorch-complete-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      In Pytorch: complete self-attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#building-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Building transformers
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-transformer-block" class="md-nav__link">
    <span class="md-ellipsis">
      The transformer block
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The transformer block">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-structure" class="md-nav__link">
    <span class="md-ellipsis">
      General structure
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#our-first-application-classification-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      Our first application: Classification transformer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Our first application: Classification transformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#input-using-the-positions" class="md-nav__link">
    <span class="md-ellipsis">
      Input: using the positions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Input: using the positions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#option-1-position-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      option 1: position embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#option-2-position-encodings" class="md-nav__link">
    <span class="md-ellipsis">
      option 2: position encodings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-generate-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Text-generate Transformers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-global-context-aware-transformer-for-language-guided-video-segmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Local-Global Context Aware Transformer for Language-Guided Video Segmentation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="about-transformer">About Transformer</h1>
<p>refer to <a href="https://peterbloem.nl/blog/transformers">TRANSFORMERS FROM SCRATCH</a></p>
<h2 id="self-attention">Self-attention</h2>
<ol>
<li>Self-attention is a sequence-to-sequence operation, 而且input vectors <span class="arithmatex">\(x_1, x_2, ...,x_t\)</span>和output vectors<span class="arithmatex">\(y_1, y_2,...,y_t\)</span>的dimension相同</li>
<li>The output vector <span class="arithmatex">\(y_i\)</span> is a weighted average over all the input vectors</li>
</ol>
<div class="arithmatex">\[
y_i = \sum_j{w_{ij}x_j}
\]</div>
<ol>
<li>So what is <span class="arithmatex">\(w_{ij}\)</span> ? The weight <span class="arithmatex">\(w_{ij}\)</span> is not a parameter, as in a normal neural net, but it is <em>derived</em> from a function over <span class="arithmatex">\(x_i\)</span> and <span class="arithmatex">\(x_j\)</span>. The simplest option for this function is the dot product:</li>
</ol>
<div class="arithmatex">\[
W'_{ij} = x_i^Tx_j
\]</div>
<ol>
<li>当然还要对 <span class="arithmatex">\(W'_{ij}\)</span> 作 Normalization，</li>
</ol>
<div class="arithmatex">\[
w_{ij} = \frac{e^{w_{ij}}}{\sum_j{e^{w'_{ij}}}}.
\]</div>
<p><img alt="img" src="https://peterbloem.nl/files/transformers/self-attention.svg" /></p>
<ol>
<li>Summary: '<em>This is the only operation in the whole architecture that propagates information between vectors. Every other operation in the transformer is applied to each vector in the input sequence without interactions between vectors.</em>'</li>
</ol>
<h2 id="understanding-why-self-attention-works">Understanding why self-attention works !!!</h2>
<h3 id="example-movie-recommendation">example: <em>movie recommendation</em></h3>
<p><strong><em>' we make the movie features and user features parameters of the model. We then ask users for a small number of movies that they like and we optimize the user features and movie features so that their dot product matches the known likes</em></strong></p>
<p><strong><em>Even though we don’t tell the model what any of the features should mean, in practice, it turns out that after training the features do actually reflect meaningful semantics about the movie content.'</em></strong></p>
<h3 id="same-with-the-self-attention-mechanism">Same with the self-attention mechanism!</h3>
<p>Self-attention 实际上是没有参数(<em>for now</em>)的, 只是对每个input vector，用它与其他所有vector求内积，求相关度，然后加权产生新的vector的一个工具。</p>
<p>那么模型怎么能在训练中趋于有效呢？模型训练时，会根据Self-attention求出的相关度符不符合实际结果，进行调整。</p>
<p>那么调整的是什么呢？<strong>input sequence</strong>.</p>
<p>假如我们现在对一串文字使用self-attension,</p>
<p><strong><em>'we simply assign each word <span class="arithmatex">\(t\)</span> in our vocabulary an embedding vector <span class="arithmatex">\(v_t\)</span>​​ (the values of which we’ll learn). This is what’s known as an embedding layer in sequence modeling'</em></strong></p>
<p>学的就是 word embedding 后的 <span class="arithmatex">\(sequence\{V_t\}\)</span></p>
<h3 id="note">Note</h3>
<p><img alt="image-20240322113023234" src="C:\Users\86198\AppData\Roaming\Typora\typora-user-images\image-20240322113023234.png" /></p>
<h2 id="in-pytorch-basic-self-attention">In Pytorch: basic self-attention</h2>
<p>See <code>basic_self_attention.py</code></p>
<h2 id="additional-tricks">Additional tricks</h2>
<blockquote>
<p>The actual self-attention used in modern transformers relies on three additional tricks. </p>
</blockquote>
<h3 id="1-queries-keys-and-values">1) Queries, keys and values</h3>
<p>在<strong>basic self-attention</strong>中，input vector <span class="arithmatex">\(x_i\)</span>​ is used in three different ways in the self attention operation:</p>
<ul>
<li>It is compared to every other vector to establish the weights for its own output <span class="arithmatex">\(𝐲_i\)</span></li>
<li>It is compared to every other vector to establish the weights for the output of the j-th vector <span class="arithmatex">\(y_j\)</span></li>
<li>It is used as part of the weighted sum to compute each output vector once the weights have been established</li>
</ul>
<p>These roles are often called the <strong>query</strong>, the <strong>key</strong> and the <strong>value</strong>
$$
q_i = W_q x_{i} \qquad k_i = W_kx_i \qquad v_i = W_vx_i \
w'<em>{ij} = \frac{q_i^Tk_j}{\sqrt{k}} \
w</em>{ij} = softmax(w'<em>{ij}) \
y_i = \sum_j{w'</em>{ij}v_i}
$$</p>
<h3 id="2-scaling-the-dot-product">2) Scaling the dot product</h3>
<h3 id="3-multi-head-attention">3) Multi-head attention</h3>
<h2 id="in-pytorch-complete-self-attention">In Pytorch: complete self-attention</h2>
<p>See <code>complete_self_attention.py</code></p>
<h2 id="building-transformers">Building <em>transformers</em></h2>
<blockquote>
<p>A transformer is not just a self-attention layer, it is an <em>architecture</em></p>
<p>The definition of the transformer architecture is vague, but here we’ll use the following definition:</p>
<blockquote>
<p>Any architecture designed to process a connected set of units—such as the tokens in a sequence or the pixels in an image—where the only interaction between units is through self-attention.</p>
</blockquote>
</blockquote>
<p><strong>Transformers</strong> 和 <strong>Convolutions</strong>一样，have a <em>standard approach to build self-attention layers up into a larger network</em>. The first step is to wrap the self-attention into a <em>block</em> that we can repeat.</p>
<h2 id="the-transformer-block">The transformer block</h2>
<h4 id="general-structure">General structure</h4>
<p><img alt="img" src="https://peterbloem.nl/files/transformers/transformer-block.svg" /></p>
<p>The block applies, in sequence: a self attention layer, layer normalization, a feed forward layer (a single MLP applied independently to each vector), and another layer normalization. Residual connections are added around both, before the normalization</p>
<blockquote>
<p>！<em>Normalization and residual connections are standard tricks used to help deep neural networks train faster and more accurately. The layer normalization is applied over the embedding dimension only.</em>  需要再去了解一些，这里就直接拿来用</p>
</blockquote>
<p><strong>Implementation:</strong> <em>also see in <code>transformers_block.py</code></em></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">heads</span><span class="p">):</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">heads</span><span class="p">)</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">k</span><span class="p">)</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>        <span class="p">)</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>        <span class="n">attended</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">attended</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>        <span class="n">fedforward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">fedforward</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
</code></pre></div>
<blockquote>
<p>We’ve made the relatively arbitrary choice of making the hidden layer of the feedforward 4 times as big as the input and output. Smaller values may work as well, and save memory, but it should be bigger than the input/output layers. </p>
<p>关于MLP 、Hidden Layer参数怎么选有点忘了，先拿来用</p>
</blockquote>
<h3 id="our-first-application-classification-transformer">Our first application: Classification transformer</h3>
<blockquote>
<p>The simplest transformer we can build is a <em>sequence classifier</em>. <em>We’ll use the IMDb sentiment classification dataset: the instances are movie reviews, tokenized into sequences of words, and the classification labels are <code>positive</code> and <code>negative</code> (indicating whether the review was positive or negative about the movie)</em></p>
</blockquote>
<p><strong>The General Idea:</strong> Use a large chain of transformer blocks to extract the information in the movie reviews. Feed it the input sequence of words from the tokenized movie reviews,  transformer blocks would produce a output sequence, then do something to it to get a single classification.</p>
<blockquote>
<p><em>The most common way to build a sequence classifier out of sequence-to-sequence layers, is to apply global average pooling to the final output sequence, and to map the result to a softmaxed class vector.</em></p>
</blockquote>
<p><img alt="img" src="https://peterbloem.nl/files/transformers/classifier.svg" /></p>
<blockquote>
<p><em>The most common way to build a sequence classifier out of sequence-to-sequence layers, is to apply global average pooling to the final output sequence, and to map the result to a softmaxed class vector.</em></p>
</blockquote>
<h4 id="input-using-the-positions">Input: using the positions</h4>
<p>之前我们提到，self-attention是 permutation invariant的（即input sequence 的vector 顺序改变，并不会影响每个vector 最后 output 出的结果），然后 transformer block 的 其他层 <em>layer normalization, a feed forward layer</em> 也都是 permutation invariant 的。这就是说，我们现在的transformer block 对于词序不同的两句话，最后输出的classification 结果是相同的。 显然对于人类语言来说，一句话中的词序变化非常影响一句话的语意。因此需要改进。</p>
<blockquote>
<p><em>Clearly, we want our state-of-the-art language model to have at least some </em><em>sensitivity to word order</em><em>, so this needs to be fixed.</em></p>
<p><em>The solution is simple: we create a second vector of equal length, that represents the position of the word in the current sentence, and add this to the word embedding. There are two options.</em></p>
</blockquote>
<h5 id="option-1-position-embeddings">option 1: position embeddings</h5>
<p><em>' We simply embed the positions like we did the words. Just like we created embedding vectors 𝐯cat and 𝐯susan, we create embedding vectors 𝐯12 and 𝐯25. Up to however long we expect sequences to get. The drawback is that we have to see sequences of every length during training, otherwise the relevant position embeddings don't get trained. The benefit is that it works pretty well, and it's easy to implement.'</em></p>
<h5 id="option-2-position-encodings">option 2: position encodings</h5>
<p><em>' Position encodings work in the same way as embeddings, except that we don't learn the position vectors, we just choose some function <span class="arithmatex">\(f:ℕ→ℝ^k\)</span>​ to map the positions to real valued vectors, and let the network figure out how to interpret these encodings. The benefit is that for a well chosen function, the network should be able to deal with sequences that are longer than those it's seen during training (it's unlikely to perform well on them, but at least we can check). The drawbacks are that the choice of encoding function is a complicated hyperparameter, and it complicates the implementation a little.'</em></p>
<blockquote>
<p>note：不太懂两者区别，为什么 position encoding 不用 学 position vector？</p>
</blockquote>
<p><strong>For the sake of simplicity, we'll use position embeddings in our implementation</strong></p>
<h3 id="text-generate-transformers">Text-generate Transformers</h3>
<h3 id="local-global-context-aware-transformer-for-language-guided-video-segmentation">Local-Global Context Aware Transformer for Language-Guided Video Segmentation</h3>
<p><img alt="alt text" src="../image.png" /></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.5cfa9459.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>